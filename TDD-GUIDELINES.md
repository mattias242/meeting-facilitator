# TDD Guidelines for Meeting Facilitator AI

## ğŸ¯ Test-Driven Development Strategy

### Overview
Detta dokument beskriver TDD-approach fÃ¶r Meeting Facilitator AI-projektet. Vi fÃ¶ljer **Red-Green-Refactor** cykeln fÃ¶r att bygga in kvalitet frÃ¥n start.

### Test Pyramid
```
    ğŸ”¬ E2E Tests (5%)
       â†‘
   ğŸ§ª Integration Tests (15%)  
       â†‘
  âœ… Unit Tests (80%)
```

## ğŸ“‹ Test Categories

### 1. Unit Tests (80%)
- **Backend**: Services, models, utilities
- **Frontend**: Hooks, utilities, components
- **Coverage**: >90% fÃ¶r critical paths

### 2. Integration Tests (15%)
- **API endpoints**: Full request/response cycles
- **Database**: Model interactions
- **WebSocket**: Connection handling

### 3. E2E Tests (5%)
- **Full meeting flow**: Create â†’ Record â†’ Analyze â†’ Protocol
- **Critical user journeys**

## ğŸ”„ TDD Workflow

### Red Phase
1. **Skriv ett failing test**
   - Beskriv exakt vad du vill implementera
   - Testet ska misslyckas med tydligt felmeddelande
   
### Green Phase  
2. **GÃ¶r testet pass**
   - Minimal implementation fÃ¶r att fÃ¥ testet att grÃ¶nas
   - Inga extra features, bara det nÃ¶dvÃ¤ndigaste
   
### Refactor Phase
3. **FÃ¶rbÃ¤ttra koden**
   - Clean code, DRY, SOLID principles
   - BehÃ¥ll alla tests grÃ¶na

## ğŸ“ Test Naming Conventions

### Backend (pytest)
```python
class TestServiceName:
    def test_method_scenario_expected_result(self):
        """Test description in Swedish."""
        pass
    
    def test_invalid_input_raises_error(self):
        """Test error handling."""
        pass
```

### Frontend (vitest)
```typescript
describe('ComponentName', () => {
  it('should behave correctly when scenario', () => {
    // Test implementation
  })
  
  it('should handle error case', () => {
    // Error handling test
  })
})
```

## ğŸ› ï¸ Test Structure

### Backend Test Structure
```
backend/tests/
â”œâ”€â”€ conftest.py              # Shared fixtures
â”œâ”€â”€ test_api/               # API endpoint tests
â”‚   â”œâ”€â”€ test_meetings.py
â”‚   â”œâ”€â”€ test_audio.py
â”‚   â””â”€â”€ test_protocols.py
â”œâ”€â”€ test_services/          # Business logic tests
â”‚   â”œâ”€â”€ test_idoarrt_service.py
â”‚   â”œâ”€â”€ test_claude_service.py
â”‚   â””â”€â”€ test_transcription_service.py
â”œâ”€â”€ test_models/            # Database model tests
â””â”€â”€ test_integration/       # Integration tests
    â””â”€â”€ test_meeting_flow.py
```

### Frontend Test Structure
```
frontend/src/
â”œâ”€â”€ test/
â”‚   â””â”€â”€ setup.ts           # Global test setup
â”œâ”€â”€ hooks/
â”‚   â””â”€â”€ *.test.ts          # Hook tests
â”œâ”€â”€ components/
â”‚   â””â”€â”€ *.test.tsx         # Component tests
â”œâ”€â”€ services/
â”‚   â””â”€â”€ *.test.ts          # Service tests
â””â”€â”€ utils/
    â””â”€â”€ *.test.ts          # Utility tests
```

## ğŸ¯ Test Coverage Goals

### Critical Components (>95%)
- IDOARRT parsing & validation
- Audio recording & upload
- Claude API integration
- Meeting state management

### Important Components (>80%)
- WebSocket connections
- Protocol generation
- UI components

### Support Components (>60%)
- Utilities
- Error handling
- Logging

## ğŸ“Š Quality Gates

### Pre-commit Checks
```bash
# Backend
ruff check .              # Linting
mypy app/                 # Type checking  
pytest --cov=app         # Test coverage

# Frontend
npm run lint              # ESLint
npm run typecheck         # TypeScript
npm run test:coverage     # Test coverage
```

### CI/CD Pipeline
- **All tests must pass**
- **Coverage targets met**
- **Security scans clear**
- **No new vulnerabilities**

## ğŸ§ª Test Data Management

### Fixtures
- **Deterministic data**: Use predefined test data
- **Isolation**: Each test independent
- **Cleanup**: Automatic teardown

### Mocking Strategy
- **External APIs**: Always mock (Claude, transcription)
- **Browser APIs**: Mock MediaRecorder, WebSocket
- **Database**: Use in-memory SQLite

## ğŸ“‹ Test Examples

### Backend Service Test
```python
def test_parse_valid_idoarrt_success(self, sample_idoarrt_markdown):
    """Test parsing valid IDOARRT markdown."""
    # When
    result = self.service.parse_idoarrt(sample_idoarrt_markdown)
    
    # Then
    assert result["intent"] == "Testa mÃ¶tesfacilitering"
    assert len(result["desired_outcomes"]) == 2
```

### Frontend Hook Test
```typescript
it('should start recording when start button is clicked', async () => {
  const user = userEvent.setup()
  render(<TestComponent onChunkReady={mockChunkReady} />)
  
  await user.click(screen.getByText('Start'))
  
  expect(screen.getByTestId('is-recording')).toHaveTextContent('true')
})
```

### Integration Test
```python
def test_full_meeting_lifecycle(self, integration_client):
    """Test complete meeting from creation to protocol."""
    # Create meeting
    # Start meeting  
    # Upload audio
    # Generate protocol
    # Verify all steps
```

## ğŸš€ Best Practices

### DO âœ…
- **Write tests first** (TDD)
- **Test one thing per test**
- **Use descriptive test names**
- **Mock external dependencies**
- **Keep tests fast and isolated**
- **Test edge cases and errors**
- **Maintain high coverage**

### DON'T âŒ
- **Don't test implementation details**
- **Don't skip tests for "simple" code**
- **Don't use shared state between tests**
- **Don't ignore flaky tests**
- **Don't mock everything indiscriminately**

## ğŸ”§ Running Tests

### Backend
```bash
cd backend
pytest                           # All tests
pytest tests/test_services/       # Service tests only
pytest --cov=app                 # With coverage
pytest -v                        # Verbose output
```

### Frontend
```bash
cd frontend
npm test                         # All tests
npm run test:ui                  # Interactive UI
npm run test:coverage            # With coverage
```

### Integration
```bash
cd backend
pytest tests/test_integration/   # Full flow tests
```

## ğŸ“ˆ Metrics & Monitoring

### Coverage Reports
- **Backend**: `coverage/` directory
- **Frontend**: `coverage/` directory  
- **CI**: Codecov integration

### Quality Metrics
- **Test count**: Track growth
- **Coverage percentage**: Maintain >80%
- **Test duration**: Keep fast
- **Flaky test rate**: Zero tolerance

## ğŸ¯ Next Steps

1. **Implement missing test coverage**
2. **Add performance tests**  
3. **Set up monitoring dashboards**
4. **Create test data generators**
5. **Automate regression testing**

---

**Remember**: "If it's not tested, it's broken." - TDD Mantra
